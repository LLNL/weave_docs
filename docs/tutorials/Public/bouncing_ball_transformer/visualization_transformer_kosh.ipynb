{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries\n",
    "**For more examples of what Kosh can do visit [GitHub Examples](https://github.com/LLNL/kosh/tree/stable/examples).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numbers import Number\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import kosh\n",
    "import math\n",
    "import statistics\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "# import horovod.tensorflow as hvd\n",
    "\n",
    "print(sys.argv[1])\n",
    "if \"-f\" in sys.argv[1]:  # Running as notebook\n",
    "    out_path = 'ball-bounce-transformer_20250114-142359'\n",
    "    use_gpu = False\n",
    "    %matplotlib inline\n",
    "else:\n",
    "    out_path = sys.argv[1]  # Running as script\n",
    "    use_gpu = True\n",
    "\n",
    "# Ensembles Initialization\n",
    "database = os.path.join(out_path, 'ensembles_output.sqlite')\n",
    "print(database)\n",
    "datastore = kosh.connect(database)\n",
    "print(\"Kosh is ready!\")\n",
    "\n",
    "# Printing Attributes and Features\n",
    "test_rec = list(datastore.find())[1]\n",
    "print('Attributes:')\n",
    "print('\\t',test_rec.list_attributes())\n",
    "print('\\n')\n",
    "print('Features Sets:')\n",
    "print('\\t',test_rec.list_features())\n",
    "time=test_rec['physics_cycle_series/time'][:]\n",
    "image_path = os.path.join(out_path, 'transformer-ball-bounce/images')\n",
    "os.makedirs(image_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "\n",
    "Create a Rank 3 Tensor for each of 60% Training, 20% Validation, and 20% Test Data.\n",
    "\n",
    "(# of Datasets, # of Time Steps per Dataset, # of Features per Time Step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize Arrays\n",
    "X_train=np.array([])\n",
    "X_val=np.array([])\n",
    "X_test=np.array([])\n",
    "\n",
    "num_datasets = len(list(datastore.find(load_type='dictionary'))) - 1 # Subtract the 'mean' dataset from the other notebook\n",
    "train_datasets = round(num_datasets*.6)\n",
    "val_datasets = round(num_datasets*.2)\n",
    "test_datasets = round(num_datasets*.2)    \n",
    "print(f'Total Datasets: {num_datasets}')\n",
    "print('Number of train, val, and test datasets:', train_datasets, val_datasets, test_datasets)\n",
    "\n",
    "for i, dataset in enumerate(datastore.find(load_type='dictionary')): # Each record is now a dataset\n",
    "\n",
    "        print(f\"----------------------Dataset #{i}: ID: {dataset['id']}----------------------\")\n",
    "        if dataset['id']=='mean':\n",
    "            continue\n",
    "\n",
    "        x_pos = dataset['curve_sets']['physics_cycle_series']['dependent']['x_pos']['value'][:]\n",
    "        y_pos = dataset['curve_sets']['physics_cycle_series']['dependent']['y_pos']['value'][:]\n",
    "        z_pos = dataset['curve_sets']['physics_cycle_series']['dependent']['z_pos']['value'][:]\n",
    "\n",
    "        # Current dataset matrix for features\n",
    "        X = pd.DataFrame([x_pos, y_pos, z_pos]).transpose()\n",
    "        X.columns=['x_pos','y_pos','z_pos']\n",
    "\n",
    "        # Concatenating the tensor\n",
    "        if i<train_datasets:\n",
    "            if X_train.size==0:\n",
    "                X_train = np.array([X.values])\n",
    "            else:\n",
    "                X_train = np.vstack((X_train,[X.values])) \n",
    "        elif i<train_datasets+val_datasets:\n",
    "            if X_val.size==0:\n",
    "                X_val = np.array([X.values ])\n",
    "            else:\n",
    "                X_val =  np.vstack((X_val,[X.values]))\n",
    "        elif i<train_datasets+val_datasets+test_datasets:\n",
    "            if X_test.size==0:\n",
    "                X_test = np.array([X.values ])\n",
    "            else:\n",
    "                X_test =  np.vstack((X_test,[X.values]))   \n",
    "\n",
    "# Plotting for acyclical data\n",
    "fig, ax = plt.subplots(nrows=3,sharex=True)\n",
    "fig.suptitle('Example of Train, Val, and Test split')\n",
    "\n",
    "ax[0].plot(time,X_train[0,:,0], label='Train')\n",
    "ax[0].plot(time,X_val[0,:,0], label='Validation')\n",
    "ax[0].plot(time,X_test[0,:,0], label='Test')\n",
    "ax[0].legend(fontsize='xx-small')\n",
    "ax[0].set_title('x_pos')\n",
    "\n",
    "ax[1].plot(time ,X_train[0,:,1], label='Train')\n",
    "ax[1].plot(time,X_val[0,:,1], label='Validation')\n",
    "ax[1].plot(time,X_test[0,:,1], label='Test')\n",
    "ax[1].legend(fontsize='xx-small')\n",
    "ax[1].set_title('y_pos')\n",
    "\n",
    "ax[2].plot(time ,X_train[0,:,2], label='Train')\n",
    "ax[2].plot(time,X_val[0,:,2], label='Validation')\n",
    "ax[2].plot(time,X_test[0,:,2], label='Test')\n",
    "ax[2].legend(fontsize='xx-small')\n",
    "ax[2].set_title('z_pos')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(os.path.join(image_path, 'example_split.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling the data\n",
    "Scaling the data so that all the features are around the same magnitude helps the model converge faster due to how the optimizers update the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_time_steps, num_features = X.shape\n",
    "print('Each whole dataset:',X.shape)\n",
    "print('\\tNumber of Time Steps in Each whole dataset:', num_time_steps) \n",
    "print('\\tNumber of Features per Time Step:', num_features) \n",
    "print('\\n')\n",
    "\n",
    "# Scaler\n",
    "scaler = MinMaxScaler()  # StandardScaler()\n",
    "\n",
    "##############\n",
    "# Train Data #\n",
    "##############\n",
    "num_datasets, num_time_steps, num_features = X_train.shape\n",
    "print('X_train:',X_train.shape)\n",
    "print('\\tNumber of Datasets:', num_datasets) \n",
    "print('\\tNumber of Time Steps per Dataset:', num_time_steps) \n",
    "print('\\tNumber of Features per Time Step:', num_features) \n",
    "print('\\n')\n",
    "\n",
    "# Reshape each feature for all datasets into one long feature for scaling\n",
    "X_train = np.reshape(X_train, newshape=(-1, num_features))\n",
    "X_train = scaler.fit_transform(X_train) # Fit AND transform only for train data\n",
    "\n",
    "# Reshape each long feature back into their own dataset\n",
    "X_train_scaled = np.reshape(X_train, newshape=(num_datasets, num_time_steps, num_features))\n",
    "\n",
    "###################\n",
    "# Validation Data #\n",
    "###################\n",
    "num_datasets, num_time_steps, num_features = X_val.shape\n",
    "print('X_val:',X_val.shape)\n",
    "print('\\tNumber of Datasets:', num_datasets) \n",
    "print('\\tNumber of Time Steps per Dataset:', num_time_steps) \n",
    "print('\\tNumber of Features per Time Step:', num_features) \n",
    "print('\\n')\n",
    "\n",
    "# Reshape each feature for all datasets into one long feature for scaling\n",
    "X_val = np.reshape(X_val, newshape=(-1, num_features))\n",
    "X_val = scaler.transform(X_val)  # Transform ONLY for validation data\n",
    "\n",
    "# Reshape each long feature back into their own dataset\n",
    "X_val_scaled = np.reshape(X_val, newshape=(num_datasets, num_time_steps, num_features))\n",
    "\n",
    "\n",
    "#############\n",
    "# Test Data #\n",
    "#############\n",
    "num_datasets, num_time_steps, num_features = X_test.shape\n",
    "print('X_test:',X_test.shape)\n",
    "print('\\tNumber of Datasets:', num_datasets) \n",
    "print('\\tNumber of Time Steps per Dataset:', num_time_steps) \n",
    "print('\\tNumber of Features per Time Step:', num_features) \n",
    "print('\\n')\n",
    "\n",
    "# Reshape each feature for all datasets into one long feature for scaling\n",
    "X_test = np.reshape(X_test, newshape=(-1, num_features))\n",
    "X_test = scaler.transform(X_test)  # Transform ONLY for test data\n",
    "\n",
    "# Reshape each long feature back into their own dataset\n",
    "X_test_scaled = np.reshape(X_test, newshape=(num_datasets, num_time_steps, num_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turning numeric data into strings\n",
    "\n",
    "We need to convert our numeric data to strings so that the transformer can process it. Each dataset consists of three features `'x'`, `'y'`, and `'z'` in a matrix of size `n_timesteps x n_features`.\n",
    "\n",
    "$$\n",
    "X_{train_{dataset=0}} = \n",
    "\\begin{bmatrix}\n",
    "    [xpos_{t=0} & ypos_{t=0} & zpos_{t=0}] \\\\\n",
    "    [xpos_{t=1} & ypos_{t=1} & zpos_{t=1}] \\\\\n",
    "    [xpos_{t=2} & ypos_{t=2} & zpos_{t=2}] \\\\\n",
    "    [\\vdots & \\vdots & \\vdots] \\\\\n",
    "    [xpos_{t=end} & ypos_{t=end} & zpos_{t=end}] \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We first transpose each dataset so that each feature becomes its own \"sentence\" which results in a new matrix of size `n_features x n_timesteps`.\n",
    "\n",
    "$$\n",
    "X_{train\\_string_{dataset=0}} = \n",
    "\\begin{bmatrix}\n",
    "    [xpos_{t=0} & xpos_{t=1} & xpos_{t=2} & \\ldots & xpos_{t=end}] \\\\\n",
    "    [ypos_{t=0} & ypos_{t=1} & ypos_{t=2} & \\ldots & ypos_{t=end}] \\\\\n",
    "    [zpos_{t=0} & zpos_{t=1} & zpos_{t=2} & \\ldots & zpos_{t=end}] \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We then append each of those sentences for each dataset into a list containing the sentences for ALL the datasets.\n",
    "\n",
    "$$\n",
    "X_{train\\_string} =  [ [xpos_\\_dataset_0{_{t=0}} \\ldots xpos_\\_dataset_0{_{t=end}}], \n",
    "[ypos_\\_dataset_0{_{t=0}} \\ldots ypos_\\_dataset_0{_{t=end}}],\n",
    "[zpos_\\_dataset_0{_{t=0}} \\ldots zpos_\\_dataset_0{_{t=end}}],\n",
    "\\ldots, \n",
    "[xpos_\\_dataset_{end}{_{t=0}} \\ldots xpos_\\_dataset_{end}{_{t=end}}],\n",
    "[ypos_\\_dataset_{end}{_{t=0}} \\ldots ypos_\\_dataset_{end}{_{t=end}}],\n",
    "[zpos_\\_dataset_{end}{_{t=0}} \\ldots zpos_\\_dataset_{end}{_{t=end}}]]\n",
    "$$\n",
    "\n",
    "We pass that list of all sentences for all datasets into `tf.keras.layers.TextVectorization()` to create a vocabulary list from the available \"words\" and convert the \"sentences\" to the vocabulary list indices.\n",
    "$$\n",
    "X_{train\\_string\\_vocab} = [ [56 \\ldots 75] , [113 \\ldots 32], [3455 \\ldots 675], \\ldots, [546 \\ldots 22] , [1123 \\ldots 132], [35 \\ldots 675] ]\n",
    "$$\n",
    "\n",
    "However, now that each number is a \"word\" the number of signficant figures causes the same number to be different words (e.g 1.23 vs 1.230). Also, when we pass in the training data, the vectorizer would never see a number or \"word\" that has different significant figures but is very close in value (e.g. 5.32456 vs 5.3245600001). One approach is to create a vocabulary based on a given range of numbers and significant figures. Below we define our significant figures `sig_figs` (to adjust our data) and timestep `dt` (to create our vocabulary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use Tensorflow's TextVectorization() method\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(standardize=None,\n",
    "                                                    output_mode='int')\n",
    "sig_figs = 3\n",
    "dt = 10**-sig_figs\n",
    "\n",
    "def features_to_sentences(X_temp_scaled):\n",
    "    X_temp_string = []\n",
    "    # Cycle through datasets and convert each of their x y z time series into its own sentence\n",
    "    for dataset in X_temp_scaled:\n",
    "        # Transpose so each x y z time series is their own sentence\n",
    "        dataset_reshape = dataset.transpose()\n",
    "        for data in dataset_reshape:\n",
    "            # Append sentences to large list of sentences\n",
    "            # No longer need to separate into x y z since Transformer predicts sentences not features\n",
    "            X_temp_string.append(\" \".join(f\"{x:.{sig_figs}f}\" for x in data))\n",
    "    return X_temp_string\n",
    "\n",
    "X_train_string = features_to_sentences(X_train_scaled)\n",
    "X_val_string = features_to_sentences(X_val_scaled)\n",
    "X_test_string = features_to_sentences(X_test_scaled)\n",
    "\n",
    "# See data\n",
    "print(f\"Original Data: {X_train_scaled.shape}\\n\", X_train_scaled[0])  # datasets x timesteps per feature x features per datasets\n",
    "print(f\"Sentence Data: {len(X_train_string)}\\n\", X_train_string[0])  # datasets x features per datasets = sentences\n",
    "\n",
    "# Create vocabulary from available data\n",
    "# vectorize_layer.adapt(X_train_string)\n",
    "arange_vocab = \" \".join(f\"{x:.{sig_figs}f}\" for x in np.arange(-2, 2+dt, dt))  # just in case out of range values\n",
    "arange_vocab += f\" {0:.{sig_figs}f}\"  # due to negative and string conversion range: 0 becomes -0\n",
    "vectorize_layer.adapt(arange_vocab)\n",
    "\n",
    "# Print vocabulary\n",
    "VOCAB_SIZE = len(vectorize_layer.get_vocabulary())\n",
    "print(\"\\n\\nTotal Number of unique 'words':\", VOCAB_SIZE)\n",
    "print(\"Vocabulary:\\n\", vectorize_layer.get_vocabulary())\n",
    "\n",
    "# Convert x y z sentences into vocabulary indices\n",
    "X_train_vectorized = vectorize_layer(X_train_string)\n",
    "X_val_vectorized = vectorize_layer(X_val_string)\n",
    "X_test_vectorized = vectorize_layer(X_test_string)\n",
    "print(\"Sentence Data in Vocabulary Index Form:\\n\", X_train_vectorized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch and Shuffle Data\n",
    "\n",
    "We create individual sentence examples the size of `MAX_TOKENS` as we don't need the whole time series as one sentence. This also allows us to create more examples from each time series.\n",
    "\n",
    "We then batch them in groups of `BATCH_SIZE` so we can train in that size accordingly.\n",
    "\n",
    "Each example within each sentence has a corresponding translation label/target that is shifted over by one so the transformer can predict the next \"word\" in the sentence. The input original language and input translation language are the same since we aren't \"translating\" languages. The output translated langauge is the one that is shifted over by one.\n",
    "\n",
    "* `X_train_examples.append(sentence[i:MAX_TOKENS+i])`\n",
    "* `X_train_labels.append(sentence[i+1:MAX_TOKENS+i+1])`\n",
    "\n",
    "We also shuffle the sentence examples to randomize the training data batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sub sentences from each sentence\n",
    "import random\n",
    "MAX_TOKENS = 128\n",
    "BATCH_SIZE = 16  # Must evenly divide into total examples or else Transformer model won't work\n",
    "\n",
    "def create_examples(X_temp_vectorized):\n",
    "    X_temp_examples = []\n",
    "    X_temp_labels = []\n",
    "    for sentence in X_temp_vectorized:\n",
    "\n",
    "        examples_per_sentence = len(sentence) - MAX_TOKENS\n",
    "        for i in range(examples_per_sentence):\n",
    "            # Input original language and input translation language are the same\n",
    "            X_temp_examples.append(sentence[i:MAX_TOKENS+i])\n",
    "            # Output translation language is shifted over by one\n",
    "            X_temp_labels.append(sentence[i+1:MAX_TOKENS+i+1])\n",
    "\n",
    "    return X_temp_examples, X_temp_labels\n",
    "\n",
    "X_train_examples, X_train_labels = create_examples(X_train_vectorized)\n",
    "X_val_examples, X_val_labels = create_examples(X_val_vectorized)\n",
    "X_test_examples, X_test_labels = create_examples(X_test_vectorized)\n",
    "\n",
    "print(\"Total Train examples: \", len(X_train_examples))\n",
    "print(\"Total Train batches: \", len(X_train_examples)/BATCH_SIZE)  # double check that these evenly divide\n",
    "print(\"Total Validation examples: \", len(X_val_examples))\n",
    "print(\"Total Validation batches: \", len(X_val_examples)/BATCH_SIZE)  # double check that these evenly divide\n",
    "print(\"Total Test examples: \", len(X_test_examples))\n",
    "print(\"Total Test batches: \", len(X_test_examples)/BATCH_SIZE)  # double check that these evenly divide\n",
    "\n",
    "def shuffle_and_batch(X_temp_examples, X_temp_labels):\n",
    "    # Shuffle them\n",
    "    zipped = list(zip(X_temp_examples, X_temp_labels))\n",
    "    random.shuffle(zipped)\n",
    "    X_temp_examples, X_temp_labels = zip(*zipped)\n",
    "\n",
    "    # Batch them\n",
    "    j = 0\n",
    "    example_batch = []\n",
    "    label_batch = []\n",
    "    X_temp_examples_batches = []\n",
    "    X_temp_labels_batches = []\n",
    "    for example, label in zip(X_temp_examples, X_temp_labels):\n",
    "        example_batch.append(example)\n",
    "        label_batch.append(label)\n",
    "        j+=1\n",
    "        if j == BATCH_SIZE:\n",
    "            X_temp_examples_batches.append(example_batch)\n",
    "            X_temp_labels_batches.append(label_batch)\n",
    "            example_batch = []\n",
    "            label_batch = []\n",
    "            j = 0\n",
    "\n",
    "    # Our input original \"language\" and input translated \"language\" are the same\n",
    "    return X_temp_examples_batches, X_temp_examples_batches, X_temp_labels_batches\n",
    "\n",
    "\n",
    "X_train_examples_batches_2, X_train_examples_batches, X_train_labels_batches = shuffle_and_batch(X_train_examples, X_train_labels)\n",
    "X_val_examples_batches_2, X_val_examples_batches, X_val_labels_batches = shuffle_and_batch(X_val_examples, X_val_labels)\n",
    "X_test_examples_batches_2, X_test_examples_batches, X_test_labels_batches = shuffle_and_batch(X_test_examples, X_test_labels)\n",
    "\n",
    "print(\"First Train Batch Length:\", len(X_train_examples_batches[0]))\n",
    "print(\"Last Train Batch Length:\", len(X_train_examples_batches[-1]))\n",
    "print(\"First Validation Batch Length:\", len(X_val_examples_batches[0]))\n",
    "print(\"Last Validation Batch Length:\", len(X_val_examples_batches[-1]))\n",
    "print(\"First Test Batch Length:\", len(X_test_examples_batches[0]))\n",
    "print(\"Last Test Batch Length:\", len(X_test_examples_batches[-1]))\n",
    "\n",
    "print(\"First Train Batch First Example:\", X_train_examples_batches[0][0])\n",
    "print(\"First Train Batch First Example First 10 Tokens:\", X_train_examples_batches[0][0][:10])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Transformer\n",
    "\n",
    "The following code cells are taken from the [Tensorflow Transformer Tutorial](https://www.tensorflow.org/text/tutorials/transformer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[The embedding and positional encoding layer](https://www.tensorflow.org/text/tutorials/transformer#the_embedding_and_positional_encoding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###########################################\n",
    "# Embedding and Positional Encoding Layer #\n",
    "###########################################\n",
    "def positional_encoding(length, depth):\n",
    "  depth = depth/2\n",
    "\n",
    "  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
    "  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
    "\n",
    "  angle_rates = 1 / (10000**depths)         # (1, depth)\n",
    "  angle_rads = positions * angle_rates      # (pos, depth)\n",
    "\n",
    "  pos_encoding = np.concatenate(\n",
    "      [np.sin(angle_rads), np.cos(angle_rads)],\n",
    "      axis=-1) \n",
    "\n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "  def __init__(self, vocab_size, d_model):\n",
    "    super().__init__()\n",
    "    self.d_model = d_model\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True) \n",
    "    self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
    "\n",
    "  def compute_mask(self, *args, **kwargs):\n",
    "    return self.embedding.compute_mask(*args, **kwargs)\n",
    "\n",
    "  def call(self, x):\n",
    "    length = tf.shape(x)[1]\n",
    "    x = self.embedding(x)\n",
    "    # This factor sets the relative scale of the embedding and positonal_encoding.\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[The base attention layer](https://www.tensorflow.org/text/tutorials/transformer#the_base_attention_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "########################\n",
    "# Base Attention Layer #\n",
    "########################\n",
    "\n",
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__()\n",
    "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "    self.add = tf.keras.layers.Add()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[The cross attention layer](https://www.tensorflow.org/text/tutorials/transformer#the_cross_attention_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "# Cross Attention Layer #\n",
    "#########################\n",
    "\n",
    "class CrossAttention(BaseAttention):\n",
    "  def call(self, x, context):\n",
    "    attn_output, attn_scores = self.mha(\n",
    "        query=x,\n",
    "        key=context,\n",
    "        value=context,\n",
    "        return_attention_scores=True)\n",
    "\n",
    "    # Cache the attention scores for plotting later.\n",
    "    self.last_attn_scores = attn_scores\n",
    "\n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[The global self-attention layer](https://www.tensorflow.org/text/tutorials/transformer#the_global_self-attention_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "# Global Attention Layer #\n",
    "##########################\n",
    "\n",
    "class GlobalSelfAttention(BaseAttention):\n",
    "  def call(self, x):\n",
    "    attn_output = self.mha(\n",
    "        query=x,\n",
    "        value=x,\n",
    "        key=x)\n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "[The causal self-attention layer](https://www.tensorflow.org/text/tutorials/transformer#the_causal_self-attention_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "# Causal Attention Layer #\n",
    "##########################\n",
    "\n",
    "class CausalSelfAttention(BaseAttention):\n",
    "  def call(self, x):\n",
    "    attn_output = self.mha(\n",
    "        query=x,\n",
    "        value=x,\n",
    "        key=x,\n",
    "        use_causal_mask = True)\n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " [The feed forward network](https://www.tensorflow.org/text/tutorials/transformer#the_feed_forward_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "######################\n",
    "# Feed Forward Layer #\n",
    "######################\n",
    "\n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    self.seq = tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),\n",
    "      tf.keras.layers.Dense(d_model),\n",
    "      tf.keras.layers.Dropout(dropout_rate)\n",
    "    ])\n",
    "    self.add = tf.keras.layers.Add()\n",
    "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.add([x, self.seq(x)])\n",
    "    x = self.layer_norm(x) \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[The encoder layer](https://www.tensorflow.org/text/tutorials/transformer#the_encoder_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#################\n",
    "# Encoder Layer #\n",
    "#################\n",
    "\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "\n",
    "    self.self_attention = GlobalSelfAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=d_model,\n",
    "        dropout=dropout_rate)\n",
    "\n",
    "    self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.self_attention(x)\n",
    "    x = self.ffn(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[The encoder](https://www.tensorflow.org/text/tutorials/transformer#the_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###############\n",
    "# THE ENCODER #\n",
    "###############\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, *, num_layers, d_model, num_heads,\n",
    "               dff, vocab_size, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.pos_embedding = PositionalEmbedding(\n",
    "        vocab_size=vocab_size, d_model=d_model)\n",
    "\n",
    "    self.enc_layers = [\n",
    "        EncoderLayer(d_model=d_model,\n",
    "                     num_heads=num_heads,\n",
    "                     dff=dff,\n",
    "                     dropout_rate=dropout_rate)\n",
    "        for _ in range(num_layers)]\n",
    "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "  def call(self, x):\n",
    "    # `x` is token-IDs shape: (batch, seq_len)\n",
    "    x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "\n",
    "    # Add dropout.\n",
    "    x = self.dropout(x)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x = self.enc_layers[i](x)\n",
    "\n",
    "    return x  # Shape `(batch_size, seq_len, d_model)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[The decoder layer](https://www.tensorflow.org/text/tutorials/transformer#the_decoder_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#################\n",
    "# Decoder Layer #\n",
    "#################\n",
    "\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self,\n",
    "               *,\n",
    "               d_model,\n",
    "               num_heads,\n",
    "               dff,\n",
    "               dropout_rate=0.1):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "\n",
    "    self.causal_self_attention = CausalSelfAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=d_model,\n",
    "        dropout=dropout_rate)\n",
    "\n",
    "    self.cross_attention = CrossAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=d_model,\n",
    "        dropout=dropout_rate)\n",
    "\n",
    "    self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "  def call(self, x, context):\n",
    "    x = self.causal_self_attention(x=x)\n",
    "    x = self.cross_attention(x=x, context=context)\n",
    "\n",
    "    # Cache the last attention scores for plotting later\n",
    "    self.last_attn_scores = self.cross_attention.last_attn_scores\n",
    "\n",
    "    x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[The decoder](https://www.tensorflow.org/text/tutorials/transformer#the_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###############\n",
    "# THE DECODER #\n",
    "###############\n",
    "\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n",
    "               dropout_rate=0.1):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n",
    "                                             d_model=d_model)\n",
    "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "    self.dec_layers = [\n",
    "        DecoderLayer(d_model=d_model, num_heads=num_heads,\n",
    "                     dff=dff, dropout_rate=dropout_rate)\n",
    "        for _ in range(num_layers)]\n",
    "\n",
    "    self.last_attn_scores = None\n",
    "\n",
    "  def call(self, x, context):\n",
    "    # `x` is token-IDs shape (batch, target_seq_len)\n",
    "    x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "    x = self.dropout(x)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x  = self.dec_layers[i](x, context)\n",
    "\n",
    "    self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
    "\n",
    "    # The shape of x is (batch_size, target_seq_len, d_model).\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[The Transformer](https://www.tensorflow.org/text/tutorials/transformer#the_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###################\n",
    "# THE TRANSFORMER #\n",
    "###################\n",
    "\n",
    "class Transformer(tf.keras.Model):\n",
    "  def __init__(self, *, num_layers, d_model, num_heads, dff,\n",
    "               input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n",
    "                           num_heads=num_heads, dff=dff,\n",
    "                           vocab_size=input_vocab_size,\n",
    "                           dropout_rate=dropout_rate)\n",
    "\n",
    "    self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n",
    "                           num_heads=num_heads, dff=dff,\n",
    "                           vocab_size=target_vocab_size,\n",
    "                           dropout_rate=dropout_rate)\n",
    "\n",
    "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    # To use a Keras model with `.fit` you must pass all your inputs in the\n",
    "    # first argument.\n",
    "    context, x  = inputs\n",
    "\n",
    "    context = self.encoder(context)  # (batch_size, context_len, d_model)\n",
    "\n",
    "    x = self.decoder(x, context)  # (batch_size, target_len, d_model)\n",
    "\n",
    "    # Final linear layer output.\n",
    "    logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n",
    "\n",
    "    try:\n",
    "      # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
    "      # b/250038731\n",
    "      del logits._keras_mask\n",
    "    except AttributeError:\n",
    "      pass\n",
    "\n",
    "    # Return the final output and the attention weights.\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting it up\n",
    "\n",
    "Now that the Transfromer and its layers have been created we can start setting it up. The only parameters that were changed from the original tutorial were the `input_vocabe_size` and `target_vocab_size` as we used a different tokenizer: `tf.keras.layers.TextVectorization()`.\n",
    "\n",
    "[Hyperparameters](https://www.tensorflow.org/text/tutorials/transformer#hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#############\n",
    "# Set it up #\n",
    "#############\n",
    "\n",
    "num_layers = 4  # The number of encoder and decoder layers\n",
    "d_model = 128  # The Positional Embedding depth (how many \"linguistic\" features to create per word)\n",
    "dff = 512  # How many nodes the feed forward network has per layer\n",
    "num_heads = 8  # The number of self-attention heads\n",
    "dropout_rate = 0.1  # Dropout rate in feed forward network\n",
    "\n",
    "transformer = Transformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    input_vocab_size=VOCAB_SIZE+1,\n",
    "    target_vocab_size=VOCAB_SIZE+1,\n",
    "    dropout_rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Set Up\n",
    "\n",
    "See [Training](https://www.tensorflow.org/text/tutorials/transformer#training) for more information.\n",
    "\n",
    "We also included [Horovod](https://horovod.ai/) which is a library that allows distributed training of datasets for multiple deep learning libraries. A user can elect to use it by uncommenting the `# GPU Utilization #` section. Some more examples are [here](https://github.com/horovod/horovod/tree/master/examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    step = tf.cast(step, dtype=tf.float32)\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "\n",
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)\n",
    "\n",
    "# Loss and metrics \n",
    "def masked_loss(label, pred):\n",
    "  mask = label != 0\n",
    "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "  loss = loss_object(label, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss.dtype)\n",
    "  loss *= mask\n",
    "\n",
    "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "  return loss\n",
    "\n",
    "\n",
    "def masked_accuracy(label, pred):\n",
    "  pred = tf.argmax(pred, axis=2)\n",
    "  label = tf.cast(label, pred.dtype)\n",
    "  match = label == pred\n",
    "\n",
    "  mask = label != 0\n",
    "\n",
    "  match = match & mask\n",
    "\n",
    "  match = tf.cast(match, dtype=tf.float32)\n",
    "  mask = tf.cast(mask, dtype=tf.float32)\n",
    "  return tf.reduce_sum(match)/tf.reduce_sum(mask)\n",
    "\n",
    "verbose = 1\n",
    "callbacks = []\n",
    "\n",
    "###################\n",
    "# GPU Utilization #\n",
    "###################\n",
    "# hvd.init()\n",
    "\n",
    "# if use_gpu:\n",
    "#     device = 'GPU'\n",
    "#     gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "#     for gpu in gpus:\n",
    "#         tf.config.experimental.set_memory_growth(gpu, True)\n",
    "#     if gpus:\n",
    "#         tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], 'GPU')\n",
    "# else:\n",
    "#     device = 'CPU'\n",
    "#     os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "# optimizer = hvd.DistributedOptimizer(optimizer)\n",
    "\n",
    "# callbacks = [\n",
    "#     # Horovod: broadcast initial variable states from rank 0 to all other processes.\n",
    "#     # This is necessary to ensure consistent initialization of all workers when\n",
    "#     # training is started with random weights or restored from a checkpoint.\n",
    "#     hvd.callbacks.BroadcastGlobalVariablesCallback(0),\n",
    "\n",
    "#     # Horovod: average metrics among workers at the end of every epoch.\n",
    "#     #\n",
    "#     # Note: This callback must be in the list before the ReduceLROnPlateau,\n",
    "#     # TensorBoard or other metrics-based callbacks.\n",
    "#     hvd.callbacks.MetricAverageCallback(),\n",
    "# ]\n",
    "\n",
    "\n",
    "# # Horovod: save checkpoints only on worker 0 to prevent other workers from corrupting them.\n",
    "# if hvd.rank() == 0:\n",
    "#     callbacks.append(tf.keras.callbacks.ModelCheckpoint('./checkpoint-{epoch}.h5'))\n",
    "\n",
    "# # Horovod: write logs on worker 0.\n",
    "# verbose = 1 if hvd.rank() == 0 else 0\n",
    "\n",
    "# print('Number of %ss: %d' % (device, hvd.size()))\n",
    "\n",
    "###########\n",
    "# Compile #\n",
    "###########\n",
    "transformer.compile(\n",
    "    loss=masked_loss,\n",
    "    optimizer=optimizer,\n",
    "    metrics=[masked_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train The Model\n",
    "\n",
    "We will now convert our Train, Validation, and Test data matrices into the [Tensorflow Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) object format so that it is compatible with the Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_batches = tf.data.Dataset.from_tensor_slices(((X_train_examples_batches, X_train_examples_batches_2), X_train_labels_batches))\n",
    "val_batches = tf.data.Dataset.from_tensor_slices(((X_val_examples_batches, X_val_examples_batches_2), X_val_labels_batches))\n",
    "test_batches = tf.data.Dataset.from_tensor_slices(((X_test_examples_batches, X_test_examples_batches_2), X_test_labels_batches))\n",
    "\n",
    "# print(X_train_examples_batches)\n",
    "# print(train_batches)\n",
    "\n",
    "###################\n",
    "# Train the model #\n",
    "###################\n",
    "\n",
    "history = transformer.fit(train_batches,\n",
    "                          epochs=20,\n",
    "                          validation_data=val_batches,\n",
    "                          callbacks=callbacks,\n",
    "                          verbose=verbose)\n",
    "\n",
    "###########################\n",
    "# Plot the learning curve #\n",
    "###########################\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(history.history['loss'], label='Train')\n",
    "ax.plot(history.history['val_loss'], label='Val')\n",
    "ax.set_xlabel('epochs')\n",
    "ax.set_ylabel('MSE')\n",
    "ax.set_title('Train and Validation Loss')\n",
    "ax.legend()\n",
    "\n",
    "fig.savefig(os.path.join(image_path, 'learning_curve.png'))\n",
    "\n",
    "##################\n",
    "# Save the model #\n",
    "##################\n",
    "model_path = os.path.join(out_path, 'transformer-ball-bounce', 'my_transformer_model.weights.h5')\n",
    "transformer.save_weights(model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "The `Translator()` class in [Run inference](https://www.tensorflow.org/text/tutorials/transformer#run_inference) had to be modified for the numerical data. The smaller modifications were mostly in using the `vectorize_layer` `tf.keras.layers.TextVectorization()` object instead of the default `tokenizer` object from the tutorial. We also created the function `get_translations()` for the different x, y, and z data.\n",
    "\n",
    "However, one large change within the `Translator.__call__()` method is that we gave the whole sentence as context to the translator before the loop. We do this so that the prediction is for the t+1 \"word\" instead of all the individual \"words\" since we already know the \"translation\" for each \"word\" in the \"sentence\". We also modified the loop so that at the end of it we shift the \"sentence\" over with the t+1 \"word\" since we now want the t+2 \"word\" and so on. \n",
    "\n",
    "This is different than the original tutorial since the original tutorial is interested in translating all the individual \"words\" within the sentence when we only want the t+1 \"word\". Thus they only give the first word for context instead of the whole sentence since at each loop it translates the next \"word\" within the sentence rather than t+1 \"word\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You can also load it for other workflows\n",
    "model_path = os.path.join(out_path, 'transformer-ball-bounce', 'my_transformer_model.weights.h5')\n",
    "transformer.load_weights(model_path)\n",
    "\n",
    "class Translator(tf.Module):\n",
    "  def __init__(self, vectorize_layer, transformer):\n",
    "    self.vectorize_layer = vectorize_layer\n",
    "    self.transformer = transformer\n",
    "\n",
    "  def __call__(self, sentence, max_length):\n",
    "    assert isinstance(sentence, tf.Tensor)\n",
    "    if len(sentence.shape) == 0:\n",
    "      sentence = sentence[tf.newaxis]\n",
    "\n",
    "    sentence = self.vectorize_layer(sentence)\n",
    "\n",
    "    encoder_input = sentence\n",
    "\n",
    "    # Don't need `[START]` and `[END]` tokens since this isn't an actual language with sentences start and end.\n",
    "    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
    "\n",
    "    # Since we already know the \"translation\" for each \"word\" in the \"sentence\" we just want to predict t+1 \"word\"\n",
    "    # We give the whole sentence as context so it only predicts the t+1 \"word\" instead of all the individual \"words\"\n",
    "    for i in range(sentence.shape[1]):  \n",
    "        output_array = output_array.write(i, tf.reshape(sentence[0,i],[-1]))\n",
    "        \n",
    "    # Keep track of t+1 \"words\" for each \"sentence\"\n",
    "    output_array_all = []\n",
    "\n",
    "    for i in tf.range(max_length): \n",
    "        if i % int(max_length*.1) == 0:\n",
    "            print(f\"Timestep {i+1} of {max_length}\")\n",
    "\n",
    "        output = tf.transpose(output_array.stack())\n",
    "        predictions = self.transformer([encoder_input, output], training=False)\n",
    "\n",
    "        # Select the last token from the `seq_len` dimension.\n",
    "        predictions = predictions[:, -1:, :] # Shape `(batch_size, 1, vocab_size)`.\n",
    "\n",
    "        predicted_id = tf.argmax(predictions, axis=-1)\n",
    "\n",
    "        # Shift sentence over with predicted_id t+1 \"words\"\n",
    "        encoder_input = tf.reshape(tf.expand_dims(tf.concat([encoder_input[0,1:],predicted_id[0]],axis=0),-1),[1,-1])\n",
    "        # Create new output_array that has shifted sentence\n",
    "        output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
    "        for j in range(encoder_input.shape[1]):\n",
    "            output_array = output_array.write(j, tf.reshape(encoder_input[0,j],[-1]))\n",
    "        # Keep track of t+1 \"words\" for each \"sentence\"\n",
    "        output_array_all.append(int(encoder_input[0,-1]))\n",
    "\n",
    "    # The output shape is `(1, tokens)`.\n",
    "    vocab = vectorize_layer.get_vocabulary()\n",
    "    text = \" \".join([vocab[each] for each in output_array_all])\n",
    "\n",
    "    tokens = output_array_all\n",
    "\n",
    "    return text, tokens\n",
    "\n",
    "\n",
    "translator = Translator(vectorize_layer, transformer)\n",
    "\n",
    "\n",
    "def print_translation(sentence, translated_text, ground_truth):\n",
    "    print(f'Input {len(sentence.split())}:\\n{sentence}')\n",
    "    print(f'Prediction {len(translated_text.split())}:\\n{translated_text}')\n",
    "    print(f'Ground truth {len(ground_truth.split())}:\\n{ground_truth}')\n",
    "    translated_text_float = [float(x) for x in translated_text.split(\" \")]\n",
    "    ground_truth_float = [float(x) for x in ground_truth.split()]\n",
    "    diff = np.abs(np.array(ground_truth_float) - np.array(translated_text_float))\n",
    "    try:\n",
    "        index = np.nonzero(diff)[0][0]\n",
    "    except:\n",
    "        index = np.nan\n",
    "    diff = \" \".join(f\"{x:.{sig_figs}f}\" for x in diff)\n",
    "    print(f'Abs(Ground truth - Prediction):\\n{diff}')\n",
    "    print(f'First non-zero index: {index}')\n",
    "\n",
    "def get_translations(X_temp_scaled):\n",
    "    \n",
    "    def sentence_translation(X_temp_scaled_i):\n",
    "        sentence = \" \".join(f\"{x:.{sig_figs}f}\" for x in X_temp_scaled_i[:MAX_TOKENS])\n",
    "        ground_truth = \" \".join(f\"{x:.{sig_figs}f}\" for x in X_temp_scaled_i[MAX_TOKENS:])\n",
    "\n",
    "        translated_text, translated_tokens = translator(tf.constant(sentence),\n",
    "                                                        max_length=len(time)-MAX_TOKENS)\n",
    "\n",
    "        print_translation(sentence, translated_text, ground_truth)\n",
    "        \n",
    "        translated = [float(x) for x in translated_text.split()]\n",
    "        \n",
    "        return translated\n",
    "    \n",
    "    # Converting \"sentence\" to list of floats\n",
    "    print(\"\\n-----x data-----\\n\")\n",
    "    translated_x = sentence_translation(X_temp_scaled[0].transpose()[0])\n",
    "    print(\"\\n-----y data-----\\n\")\n",
    "    translated_y = sentence_translation(X_temp_scaled[0].transpose()[1])\n",
    "    print(\"\\n-----z data-----\\n\")\n",
    "    translated_z = sentence_translation(X_temp_scaled[0].transpose()[2])\n",
    "\n",
    "    return translated_x, translated_y, translated_z\n",
    "\n",
    "print(\"\\n\\n----------Train Data:----------\\n\\n\")\n",
    "translated_x_train, translated_y_train, translated_z_train = get_translations(X_train_scaled)\n",
    "print(\"\\n\\n----------Validation Data:----------\\n\\n\")\n",
    "translated_x_val, translated_y_val, translated_z_val = get_translations(X_val_scaled)\n",
    "print(\"\\n\\n----------Test Data:----------\\n\\n\")\n",
    "translated_x_test, translated_y_test, translated_z_test = get_translations(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Plotting\n",
    "\n",
    "Here we plot the predictions of the transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"ORIGINAL SCALED:\\n\")\n",
    "print(X_train_scaled[0])\n",
    "original_train = scaler.inverse_transform(X_train_scaled[0])\n",
    "print(\"ORIGINAL UNSCALED:\\n\")\n",
    "print(original_train)\n",
    "\n",
    "original_val = scaler.inverse_transform(X_val_scaled[0])\n",
    "original_test = scaler.inverse_transform(X_test_scaled[0])\n",
    "\n",
    "X_predicted_train = np.array([translated_x_train,translated_y_train,translated_z_train]).transpose()\n",
    "X_whole_train_unscaled = scaler.inverse_transform(X_predicted_train)\n",
    "print(f\"PREDICTED UNSCALED len(time) - {len(time)} - MAX_TOKENS: {MAX_TOKENS}: {len(time) - MAX_TOKENS}\\n\")\n",
    "# print(X_whole_train_unscaled)}\n",
    "\n",
    "X_predicted_val = np.array([translated_x_val,translated_y_val,translated_z_val]).transpose()\n",
    "X_whole_val_unscaled = scaler.inverse_transform(X_predicted_val)\n",
    "\n",
    "X_predicted_test = np.array([translated_x_test,translated_y_test,translated_z_test]).transpose()\n",
    "X_whole_test_unscaled = scaler.inverse_transform(X_predicted_test)\n",
    "\n",
    "#######################\n",
    "# Plot the prediction #\n",
    "#######################\n",
    "\n",
    "fig, ax = plt.subplots(nrows=3,ncols=3,sharex=True,figsize=(9,9))\n",
    "fig.suptitle('Whole Prediction')\n",
    "\n",
    "ax[0,0].plot(time,original_train[:,0], label='Original')\n",
    "ax[0,0].plot(time[0:MAX_TOKENS],original_train[0:MAX_TOKENS,0], label='Initial Guess')\n",
    "ax[0,0].plot(time[MAX_TOKENS:], X_whole_train_unscaled[:,0], label='Whole Prediction')\n",
    "ax[0,1].plot(time,original_val[:,0], label='Original')\n",
    "ax[0,1].plot(time[0:MAX_TOKENS],original_val[0:MAX_TOKENS,0], label='Initial Guess')\n",
    "ax[0,1].plot(time[MAX_TOKENS:], X_whole_val_unscaled[:,0], label='Whole Prediction')\n",
    "ax[0,2].plot(time,original_test[:,0], label='Original')\n",
    "ax[0,2].plot(time[0:MAX_TOKENS],original_test[0:MAX_TOKENS,0], label='Initial Guess')\n",
    "ax[0,2].plot(time[MAX_TOKENS:], X_whole_test_unscaled[:,0], label='Whole Prediction')\n",
    "ax[0,0].legend(fontsize='xx-small')\n",
    "ax[0,0].set_title('Train x_pos')\n",
    "ax[0,1].legend(fontsize='xx-small')\n",
    "ax[0,1].set_title('Validation x_pos')\n",
    "ax[0,2].legend(fontsize='xx-small')\n",
    "ax[0,2].set_title('Test x_pos')\n",
    "\n",
    "ax[1,0].plot(time,original_train[:,1], label='Original')\n",
    "ax[1,0].plot(time[0:MAX_TOKENS],original_train[0:MAX_TOKENS,1], label='Initial Guess')\n",
    "ax[1,0].plot(time[MAX_TOKENS:], X_whole_train_unscaled[:,1], label='Whole Prediction')\n",
    "ax[1,1].plot(time,original_val[:,1], label='Original')\n",
    "ax[1,1].plot(time[0:MAX_TOKENS],original_val[0:MAX_TOKENS,1], label='Initial Guess')\n",
    "ax[1,1].plot(time[MAX_TOKENS:], X_whole_val_unscaled[:,1], label='Whole Prediction')\n",
    "ax[1,2].plot(time,original_test[:,1], label='Original')\n",
    "ax[1,2].plot(time[0:MAX_TOKENS],original_test[0:MAX_TOKENS,1], label='Initial Guess')\n",
    "ax[1,2].plot(time[MAX_TOKENS:], X_whole_test_unscaled[:,1], label='Whole Prediction')\n",
    "ax[1,0].legend(fontsize='xx-small')\n",
    "ax[1,0].set_title('Train y_pos')\n",
    "ax[1,1].legend(fontsize='xx-small')\n",
    "ax[1,1].set_title('Validation y_pos')\n",
    "ax[1,2].legend(fontsize='xx-small')\n",
    "ax[1,2].set_title('Test y_pos')\n",
    "\n",
    "ax[2,0].plot(time,original_train[:,2], label='Original')\n",
    "ax[2,0].plot(time[0:MAX_TOKENS],original_train[0:MAX_TOKENS,2], label='Initial Guess')\n",
    "ax[2,0].plot(time[MAX_TOKENS:], X_whole_train_unscaled[:,2], label='Whole Prediction')\n",
    "ax[2,1].plot(time,original_val[:,2], label='Original')\n",
    "ax[2,1].plot(time[0:MAX_TOKENS],original_val[0:MAX_TOKENS,2], label='Initial Guess')\n",
    "ax[2,1].plot(time[MAX_TOKENS:], X_whole_val_unscaled[:,2], label='Whole Prediction')\n",
    "ax[2,2].plot(time,original_test[:,2], label='Original')\n",
    "ax[2,2].plot(time[0:MAX_TOKENS],original_test[0:MAX_TOKENS,2], label='Initial Guess')\n",
    "ax[2,2].plot(time[MAX_TOKENS:], X_whole_test_unscaled[:,2], label='Whole Prediction')\n",
    "ax[2,0].legend(fontsize='xx-small')\n",
    "ax[2,0].set_title('Train z_pos')\n",
    "ax[2,1].legend(fontsize='xx-small')\n",
    "ax[2,1].set_title('Validation z_pos')\n",
    "ax[2,2].legend(fontsize='xx-small')\n",
    "ax[2,2].set_title('Test z_pos')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(os.path.join(image_path, 'whole_prediction.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weave-demos",
   "language": "python",
   "name": "weave-demos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
