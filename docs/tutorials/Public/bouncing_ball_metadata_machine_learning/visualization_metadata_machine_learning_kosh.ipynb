{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries\n",
    "**For more examples of what Kosh can do visit [GitHub Examples](https://github.com/LLNL/kosh/tree/stable/examples).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numbers import Number\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import kosh\n",
    "import math\n",
    "import statistics\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "print(sys.argv[1])\n",
    "if \"-f\" in sys.argv[1]:  # Running as notebook\n",
    "    out_path = 'ball-bounce-metadata-machine-learning_20250205-150210'\n",
    "    use_gpu = False\n",
    "    %matplotlib inline\n",
    "else:\n",
    "    out_path = sys.argv[1]  # Running as script\n",
    "    use_gpu = True\n",
    "\n",
    "# Ensembles Initialization\n",
    "database = os.path.join(out_path, 'ensembles_output.sqlite')\n",
    "print(database)\n",
    "datastore = kosh.connect(database)\n",
    "print(\"Kosh is ready!\")\n",
    "\n",
    "# Printing Attributes and Features\n",
    "test_rec = list(datastore.find())[1]\n",
    "print('Attributes:')\n",
    "print('\\t',test_rec.list_attributes())\n",
    "print('\\n')\n",
    "print('Features Sets:')\n",
    "print('\\t',test_rec.list_features())\n",
    "time=test_rec['physics_cycle_series/time'][:]\n",
    "image_path = os.path.join(out_path, 'metadata-machine-learning-ball-bounce/images')\n",
    "os.makedirs(image_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "\n",
    "As mentioned in the `README.md`, if one is interested in predicting the final value rather than the transient behavior, one can use this metadata analysis instead of an LSTM or Transformer. Instead of loading our time series data for each run, we will load the metadata associated for each run. The metadata is known as Kosh dataset attributes. Each dataset can have its own attributes but we can extract all the attributes for all datasets into a Pandas DataFrame using `store.to_dataframe()`. The metadata for these runs was added in the `dsv_to_kosh.py` file in the `ingest-ball-bounce` step of the workflow.  See [kosh/examples/Example_Simulation_Workflow.ipynb](https://github.com/LLNL/kosh/blob/stable/examples/Example_Simulation_Workflow.ipynb) for more information on how to add metadata, update it, and extract it.\n",
    "\n",
    "Below we see a dataframe of size `n_datasets x n_attributes` where `n_datasets` is how many simulations we ran in the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = datastore.to_dataframe()\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Data\n",
    "\n",
    "We will extract our features and labels of interest from the dataframe above since `store.to_dataframe()` also includes other metadata by default. The features will be the initial conditions for the simulations `['x_pos_initial', 'x_vel_initial', 'y_pos_initial', 'y_vel_initial', 'z_pos_initial', 'z_vel_initial']` and the label will be the final x position `['x_pos_final']` when the simulation finished. We use SciKit Learn's `train_test_split()` method to split the data into train, validation, and test data. We also use Seaborn's `pairplot()` to quickly plot the features and labels in the Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extracting necessary data\n",
    "features = ['x_pos_initial', 'x_vel_initial', 'y_pos_initial', 'y_vel_initial', 'z_pos_initial', 'z_vel_initial']\n",
    "labels = ['x_pos_final']\n",
    "df_original = df[features + labels].copy()\n",
    "df_original_features = df_original[features].copy()\n",
    "df_original_labels = df_original[labels].copy()\n",
    "\n",
    "\n",
    "# Splitting data\n",
    "df_train_features, df_test_features, df_train_labels, df_test_labels = train_test_split(df_original_features, df_original_labels, test_size=0.2, random_state=42)\n",
    "df_train_features, df_validation_features, df_train_labels, df_validation_labels = train_test_split(df_train_features, df_train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print(df_train_features.head())\n",
    "\n",
    "print(f\"Train Size features: {df_train_features.shape} and labels: {df_train_labels.shape}\")\n",
    "print(f\"Validation Size features: {df_validation_features.shape} and labels: {df_validation_labels.shape}\")\n",
    "print(f\"Test Size features: {df_test_features.shape} and labels: {df_test_labels.shape}\")\n",
    "\n",
    "# Temporarily combine features and labels to see all the data on the same plot\n",
    "df_train_temp = pd.concat([df_train_features, df_train_labels], axis=1)\n",
    "sns.pairplot(df_train_temp)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling the data\n",
    "Scaling the data so that all the features are around the same magnitude helps the model converge faster due to how the optimizers update the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Train Data\n",
    "df_train_features_scaled = scaler.fit_transform(df_train_features) # Fit AND transform only for train data\n",
    "\n",
    "# Validation Data\n",
    "df_validation_features_scaled = scaler.transform(df_validation_features) # Transform ONLY for validation data\n",
    "\n",
    "# Test Data\n",
    "df_test_features_scaled = scaler.transform(df_test_features) # Transform ONLY for test data\n",
    "\n",
    "print(\"Not Scaled:\\n\",df_train_features.head())\n",
    "print(\"Scaled:\\n\",df_train_features_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turning Pandas DataFrames into Matricies\n",
    "\n",
    "Now we will convert our Pandas DataFrames into matricies so the Machine Learning algorithms can process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = df_train_features_scaled  # Already in matrix form thanks to scaler in previous code cell\n",
    "y_train = df_train_labels.to_numpy()\n",
    "\n",
    "X_validation = df_validation_features_scaled  # Already in matrix form thanks to scaler in previous code cell\n",
    "y_validation = df_validation_labels.to_numpy()\n",
    "\n",
    "X_test = df_test_features_scaled  # Already in matrix form thanks to scaler in previous code cell\n",
    "y_test = df_test_labels.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train The Model\n",
    "\n",
    "We will now train our model using `sklearn.linear_model.LinearRegression()` by using `sklearn.linear_model.LinearRegression.fit()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LinReg = LinearRegression()\n",
    "LinReg.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "Now that our model is trained, we can calculate the score of Coefficient of Determination $R^2$ `sklearn.linear_model.LinearRegression.score()` for our train, validation, and test data. We can also see what the model will infer/predict for the final x position `['x_pos_final']` given the features `['x_pos_initial', 'x_vel_initial', 'y_pos_initial', 'y_vel_initial', 'z_pos_initial', 'z_vel_initial']` using `sklearn.linear_model.LinearRegression.predict()`. We also calculate the residuals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_r_2 = LinReg.score(X_train, y_train)\n",
    "y_train_predict = LinReg.predict(X_train)\n",
    "residuals_train = y_train - y_train_predict\n",
    "\n",
    "validation_r_2 = LinReg.score(X_validation, y_validation)\n",
    "y_validation_predict = LinReg.predict(X_validation)\n",
    "residuals_validation = y_validation - y_validation_predict\n",
    "\n",
    "test_r_2 = LinReg.score(X_test, y_test)\n",
    "y_test_predict = LinReg.predict(X_test)\n",
    "residuals_test = y_test - y_test_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Plotting\n",
    "\n",
    "We create three plots each for the train, vaidation, and test data. We get a pretty high $R^2$ which is great.\n",
    "\n",
    "* `y vs y_predict` shows perfect prediction if the data points are along the slope of 1 line. We also label it with the $R^2$ value which shows perfect prediction if the value is 1.\n",
    "* `residuals vs y_predict` values should be close to 0 meaning predictions `y_predict` were not far off from the true values `y`\n",
    "* `residual distribution` same as above but from different angle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#######################\n",
    "# Plot the prediction #\n",
    "#######################\n",
    "\n",
    "fig, ax = plt.subplots(nrows=3,ncols=3,figsize=(9,9))\n",
    "fig.suptitle('Whole Prediction')\n",
    "\n",
    "ax[0,0].scatter(y_train_predict,y_train, label = f\"$R^2={train_r_2:.5f}\")\n",
    "ax[0,1].scatter(y_validation_predict,y_validation, label = f\"$R^2={validation_r_2:.5f}$\")\n",
    "ax[0,2].scatter(y_test_predict,y_test, label = f\"$R^2={test_r_2:.5f}\")\n",
    "ax[0,0].legend(fontsize='xx-small')\n",
    "ax[0,0].set_title('Train y vs y_predict')\n",
    "ax[0,1].legend(fontsize='xx-small')\n",
    "ax[0,1].set_title('Validation y vs y_predict')\n",
    "ax[0,2].legend(fontsize='xx-small')\n",
    "ax[0,2].set_title('Test y vs y_predict')\n",
    "\n",
    "ax[1,0].scatter(y_train_predict,residuals_train, label = f\"$R^2={train_r_2:.5f}\")\n",
    "ax[1,1].scatter(y_validation_predict,residuals_validation, label = f\"$R^2={validation_r_2:.5f}$\")\n",
    "ax[1,2].scatter(y_test_predict,residuals_test, label = f\"$R^2={test_r_2:.5f}\")\n",
    "ax[1,0].legend(fontsize='xx-small')\n",
    "ax[1,0].set_title('Train residuals vs y_predict')\n",
    "ax[1,1].legend(fontsize='xx-small')\n",
    "ax[1,1].set_title('Validation residuals vs y_predict')\n",
    "ax[1,2].legend(fontsize='xx-small')\n",
    "ax[1,2].set_title('Test residuals vs y_predict')\n",
    "\n",
    "ax[2,0].hist(residuals_train, label = f\"$R^2={train_r_2:.5f}\")\n",
    "ax[2,1].hist(residuals_validation, label = f\"$R^2={validation_r_2:.5f}$\")\n",
    "ax[2,2].hist(residuals_test, label = f\"$R^2={test_r_2:.5f}\")\n",
    "ax[2,0].legend(fontsize='xx-small')\n",
    "ax[2,0].set_title('Train residual distribution')\n",
    "ax[2,1].legend(fontsize='xx-small')\n",
    "ax[2,1].set_title('Validation residual distribution')\n",
    "ax[2,2].legend(fontsize='xx-small')\n",
    "ax[2,2].set_title('Test residual distribution')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(os.path.join(image_path, 'whole_prediction.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weave-demos",
   "language": "python",
   "name": "weave-demos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
